{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Propagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOION0pYQEGL"
      },
      "source": [
        "## 예제로 배우는 파이토치(Pytorch)\r\n",
        "\r\n",
        "### 특징\r\n",
        "\r\n",
        "- Numpy와 유사하지만 GPU 상에서 실행 가능한 N차원 Tensor\r\n",
        "- 신경망을 구성하고 학습하는 과정에서의 자동 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjvEyWDPOND"
      },
      "source": [
        "## Pytorch Network Training\r\n",
        "\r\n",
        "### Input\r\n",
        "\r\n",
        "1. Independent variable realization\r\n",
        "2. Initial values of all weights \r\n",
        "3. Forward function (network)  \r\n",
        "4. Dependent variable realization\r\n",
        "5. Loss function\r\n",
        "\r\n",
        "\r\n",
        "### Output\r\n",
        "\r\n",
        "1. Updated all weights \r\n",
        "\r\n",
        "### Process\r\n",
        "For $i$ = 1:  \r\n",
        "1. Calculate Forward values using Initial values(2), Forward function(3) and Independent variable realization(1) \r\n",
        "2. Calculate Loss values using Forward values, Loss function(5) and Dependent variable realization(4) \r\n",
        "3. Calculate Gradient\r\n",
        "4. Update all weights\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O672QnfmPYQ6"
      },
      "source": [
        "## Calculate Gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjigLVCWM_Qz"
      },
      "source": [
        "\r\n",
        "\r\n",
        "### 1) Data Matrix\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$ X = \\begin{bmatrix} x_{11} &...& x_{1p} \\\\ & \\vdots & \\\\ x\r\n",
        "_{n1}&  ... & x_{np}  \\end{bmatrix}  $ : $n \\times p$$\r\n",
        "\r\n",
        "<br>\r\n",
        "$$ Y = \\begin{bmatrix} y_{11} &...& y_{1k} \\\\ & \\vdots & \\\\ y\r\n",
        "_{n1}&  ... & y_{nk}  \\end{bmatrix}  $ : $n \\times k$$\r\n",
        "<br>\r\n",
        "\r\n",
        "$$where,$$\r\n",
        "<br>\r\n",
        "\r\n",
        "$$y_{ij} \\in \\{ 0,1 \\}$$\r\n",
        "\r\n",
        "$$and$$ \r\n",
        "$$\\sum_{j=1}^k y_{ij} = 1$$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$ W_1 = \\begin{bmatrix} w_{111} & w_{112} &...& w_{11h} \\\\ & & \\vdots & \\\\ w\r\n",
        "_{1p1}& w_{1p2} & ... & w_{1ph}  \\end{bmatrix} : p \\times h$$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$ W_2 = \\begin{bmatrix} w_{211} & w_{212} &...& w_{21k} \\\\& & \\vdots & \\\\ w_{2h1}& w_{2h2} & ... & w_{2hk}  \\end{bmatrix}  : h \\times k$$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "### 2) Feed Forward\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$ H = X \\cdot W_1$ : $n \\times h$$\r\n",
        "\r\n",
        "$$ H_{relu} = ReLU(H) $ : $n \\times h$$\r\n",
        "\r\n",
        "$$ Y_{pred} = H_{relu} \\cdot W_2 $ : $n \\times k$$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "### 3) Back Propagation\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$ l(W_1, W_2) =  Tr[(Y_{pred} - Y )^T (Y_{pred} - Y)] = \\sum_{i=1}^{n} \\sum_{j=1}^{k} (y_{ij}^{pred} -y_{ij} )^2    : Scalar $$\r\n",
        "\r\n",
        "</center>\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$W_{2}^{(t)} = W_{2}^{(t-1)} - \\eta {\\partial l(W_1, W_2) \\over \\partial W_{2} }$$ \r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "- grad_w2\r\n",
        "\r\n",
        "$${\\partial l(W_1, W_2) \\over \\partial W_{2} } =  \\begin{bmatrix} {\\partial l(W_1, W_2) \\over \\partial w_{211} } & {\\partial l(W_1, W_2) \\over \\partial w_{212} } &...&   {\\partial l(W_1, W_2) \\over \\partial w_{21k} } \\\\& & \\vdots & \\\\ {\\partial l(W_1, W_2) \\over \\partial w_{2h1} }& {\\partial l(W_1, W_2) \\over \\partial w_{2h2} } & ... &  {\\partial l(W_1, W_2) \\over \\partial w_{2hk} }  \\end{bmatrix}  :h \\times k $$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "#### [Chain Rule]\r\n",
        "\r\n",
        "$${\\partial l(W_1, W_2) \\over \\partial w_{2im} } =  {\\partial l(W_1, W_2) \\over \\partial y_{im}^{pred}} \\cdot {\\partial y_{im}^{pred} \\over \\partial w_{2im}}  $$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$${\\partial y_{im}^{pred} \\over \\partial w_{2jm}} = h_{ij}^{relu} ,  \\forall m \\in \\{1, ..., k\\}$$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "- grad_y_pred(usually calcuated by using a numerical differentiation)\r\n",
        "\r\n",
        "$$  2(Y_{pred} - Y) \r\n",
        "= \\begin{bmatrix} 2(y_{11}^{pred} - y_{11}) & 2(y_{12}^{pred} - y_{12}) &...&   2(y_{1k}^{pred} - y_{1k}) \\\\& & \\vdots & \\\\ 2(y_{n1}^{pred} - y_{n1}) & 2(y_{n2}^{pred} - y_{n2}) & ... & 2(y_{nk}^{pred} - y_{nk}) \\end{bmatrix}\r\n",
        "= \\begin{bmatrix} {\\partial l(W_1, W_2) \\over \\partial y_{11}^{pred} } & {\\partial l(W_1, W_2) \\over \\partial y_{12}^{pred} } &...&   {\\partial l(W_1, W_2) \\over \\partial y_{1k}^{pred} } \\\\& & \\vdots & \\\\ {\\partial l(W_1, W_2) \\over  \\partial y_{n1}^{pred} }& {\\partial l(W_1, W_2) \\over \\partial y_{n2}^{pred}} & ... &  {\\partial l(W_1, W_2) \\over \\partial y_{nk}^{pred} }  \\end{bmatrix} : n \\times  k$$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "- h_relu\r\n",
        "\r\n",
        "$$ H_{relu} \r\n",
        "= \\begin{bmatrix} h_{11}^{relu} & h_{12}^{relu} &...&   h_{1h}^{relu} \\\\& & \\vdots & \\\\ h_{n1}^{relu} & h_{n2}^{relu} &...&   h_{nh}^{relu} \\end{bmatrix}: n \\times h\r\n",
        "$$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$\\Rightarrow$ we can obtain grad_w2 by h_relu.T.dot(grad_y_pred) in below python code.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "<br>\r\n",
        "\r\n",
        "$$W_{1}^{(t)} = W_{1}^{(t-1)} + \\eta {\\partial l(W_1, W_2) \\over \\partial W_{1} }$$\r\n",
        "\r\n",
        "$${\\partial l(W_1, W_2) \\over \\partial W_1} =  [{\\partial H \\over \\partial W_1}]^T \\cdot  { H_{relu} \\over \\partial H} ( {\\partial l(W_1, W_2) \\over \\partial Y_{pred}} \\cdot [{\\partial Y_{pred}  \\over \\partial H_{relu}}]^T ) = [{\\partial H \\over \\partial W_1}]^T \\cdot  { H_{relu} \\over \\partial H} = [{\\partial H \\over \\partial W_1}]^T \\cdot grad_h\r\n",
        "  $$\r\n",
        "\r\n",
        "\r\n",
        "$\\Rightarrow$ We only define the grad_h_relu as backword function in activation class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1v0UxWM-sH"
      },
      "source": [
        "# Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDS1x59kQPv3",
        "outputId": "c412e63d-92a0-481c-d515-3cdac39657ea"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다.\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "\r\n",
        "N, D_in, H, D_out = 64,1000,100,10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다\r\n",
        "\r\n",
        "x = np.random.randn(N, D_in)\r\n",
        "y = np.random.randn(N, D_out)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "\r\n",
        "w1 = np.random.randn(D_in , H)\r\n",
        "w2 = np.random.randn(H, D_out)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "\r\n",
        "for t in range(500):\r\n",
        "  # 순전파 단계 : 예측값 y를 계산합니다.\r\n",
        "  h = x.dot(w1) # N * H\r\n",
        "  h_relu = np.maximum(h,0) # N*H\r\n",
        "  y_pred = h_relu.dot(w2) # N*D_out\r\n",
        "\r\n",
        "  # 손실(loss)을 계산하고 출력합니다.\r\n",
        "  loss = np.square(y_pred - y).sum() # N * D_out\r\n",
        "  if t % 100 == 99:\r\n",
        "    print(t, loss)\r\n",
        "\r\n",
        "  # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "  grad_y_pred = 2.0 * (y_pred - y)  # N * D_out\r\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred) # H * D_out\r\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T) # N * H\r\n",
        "  grad_h = grad_h_relu.copy() # N * H\r\n",
        "  grad_h[h < 0] = 0\r\n",
        "  grad_w1 = x.T.dot(grad_h) # D_in * H\r\n",
        "\r\n",
        "  # 가중치를 갱신합니다.\r\n",
        "  w1 -= learning_rate * grad_w1\r\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 433.71103070297227\n",
            "199 3.1559642286243528\n",
            "299 0.033146207348907226\n",
            "399 0.00038765745433218225\n",
            "499 4.774241528752624e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGtBTMiSRR96"
      },
      "source": [
        "# PyTorch: Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBnJwjWw9y2",
        "outputId": "d757a465-d242-40a9-8a2d-9c57c1d3132b"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\r\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\r\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\r\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "for t in range(500):\r\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\r\n",
        "    h = x.mm(w1)\r\n",
        "    h_relu = h.clamp(min=0)\r\n",
        "    y_pred = h_relu.mm(w2)\r\n",
        "\r\n",
        "    # 손실(loss)을 계산하고 출력합니다.\r\n",
        "    loss = (y_pred - y).pow(2).sum().item()\r\n",
        "    if t % 100 == 99:\r\n",
        "        print(t, loss)\r\n",
        "\r\n",
        "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\r\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\r\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\r\n",
        "    grad_h = grad_h_relu.clone()\r\n",
        "    grad_h[h < 0] = 0\r\n",
        "    grad_w1 = x.t().mm(grad_h)\r\n",
        "\r\n",
        "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\r\n",
        "    w1 -= learning_rate * grad_w1\r\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 613.2025146484375\n",
            "199 6.058787822723389\n",
            "299 0.10109394788742065\n",
            "399 0.0023142341524362564\n",
            "499 0.000183345444384031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQtZnIkWMLS7"
      },
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4M1n09hJSry"
      },
      "source": [
        "## Pytorch Network Training\r\n",
        "\r\n",
        "### Input\r\n",
        "\r\n",
        "1. Independent variable realization\r\n",
        "2. Initial values of all weights \r\n",
        "3. Forward function (network)  \r\n",
        "4. Dependent variable realization\r\n",
        "5. Loss function\r\n",
        "\r\n",
        "\r\n",
        "### Output\r\n",
        "\r\n",
        "1. Updated all weights \r\n",
        "\r\n",
        "### Process\r\n",
        "For $i$ = 1:  \r\n",
        "1. Calculate Forward values using Initial values(2), Forward function(3) and Independent variable realization(1) \r\n",
        "2. Calculate Loss values using Forward values, Loss function(5) and Dependent variable realization(4) \r\n",
        "3. Calculate Gradient\r\n",
        "4. Update all weights\r\n",
        "\r\n",
        "End\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fzCd4me6qYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e368a723-e8ce-43c3-f473-505df4769dc6"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "N,D_in,H,D_out = 64,1000, 100, 10\r\n",
        "\r\n",
        "x = torch.randn(N,D_in, device= device, dtype = dtype) # requires_grad = False (default): don't calculate gradient\r\n",
        "y = torch.randn(N,D_out, device = device, dtype=dtype) # requires_grad = False (default) \r\n",
        "\r\n",
        "w1 = torch.randn(D_in,H, device=device, dtype=dtype, requires_grad = True) # requires_grad = True: calculate gradient\r\n",
        "w2 = torch.randn(H,D_out, device=device, dtype=dtype, requires_grad=True)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "for t in range(500):\r\n",
        "    # forward\r\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\r\n",
        "\r\n",
        "    # loss\r\n",
        "    loss = (y_pred - y).pow(2).sum()\r\n",
        "    if t % 100 == 99:\r\n",
        "        print(t,loss.item())\r\n",
        "\r\n",
        "    # backward\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # Manually weight update\r\n",
        "    with torch.no_grad():\r\n",
        "        w1 -= learning_rate * w1.grad\r\n",
        "        w2 -= learning_rate * w2.grad\r\n",
        "\r\n",
        "        # After updating gradient, we have to make gradient zero.\r\n",
        "        w1.grad.zero_()\r\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 327.573974609375\n",
            "199 1.3231446743011475\n",
            "299 0.008650809526443481\n",
            "399 0.00019808311481028795\n",
            "499 3.4747117751976475e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foB8ClI-MZMO"
      },
      "source": [
        "# Pytorch: 새 autograd 함수 정의하기\r\n",
        "내부적으로, autograd의 기본 연산자는 실제로 Tensor를 조작하는 2개의 함수이다. `forward` 함수는 입력 Tensor로부터 출력 Tensor를 계산합니다.\r\n",
        "`backward` 함수는 어떤 스칼라 값에 대한 출력 Tensor의 변화도를 전달받고, 동일한 스칼라 값에 대한 입력 Tensor의 변화도를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vlfWZxMSF7"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# Define Activation Function\r\n",
        "class MyReLU(torch.autograd.Function):\r\n",
        "\r\n",
        "  @staticmethod # decorator \r\n",
        "  def forward(ctx, input):\r\n",
        "    '''\r\n",
        "    순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\r\n",
        "    ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에\r\n",
        "    사용합니다. ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한\r\n",
        "    객체도 저장(cache)해 둘 수 있습니다.\r\n",
        "    '''\r\n",
        "    ctx.save_for_backward(input)\r\n",
        "    return input.clamp(min=0)\r\n",
        "  \r\n",
        "  @staticmethod\r\n",
        "  def backward(ctx, grad_output):\r\n",
        "    '''\r\n",
        "    역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에 대한 손실의 변화도를 계산합니다.\r\n",
        "    '''\r\n",
        "    input, = ctx.saved_tensors\r\n",
        "    grad_input = grad_output.clone()\r\n",
        "    grad_input[input<0] =0 # grad_h\r\n",
        "    return grad_input\r\n",
        "  \r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다.\r\n",
        "# H는 은닉층의 차원이며, D_out 출력 차원입니다.\r\n",
        "\r\n",
        "N, D_in, H, D_out = 64,1000,100,10\r\n",
        "\r\n",
        "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\r\n",
        "x= torch.randn(N,D_in, device= device, dtype=dtype)\r\n",
        "y= torch.randn(N,D_out, device = device, dtype=dtype)\r\n",
        "\r\n",
        "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\r\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\r\n",
        "w2 = torch.randn(H,D_out, device=device, dtype=dtype,requires_grad=True)\r\n",
        "# 가중치만 밖에 나가 있으면 됨?\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "for t in range(500):\r\n",
        "  # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\r\n",
        "  relu = MyReLU.apply\r\n",
        "\r\n",
        "  # 순전파 단계 : Tensor 연산을 사용하여 예상되는 y 값을 계산합니다;\r\n",
        "  # 사용자 정의 autograd 연산을 사용하여 Relu 를 계산합니다.\r\n",
        "  y_pred = relu(x.mm(w1)).mm(w2)\r\n",
        "\r\n",
        "  # 손실을 계산하고 출력합니다.\r\n",
        "  loss = (y_pred - y).pow(2).sum()\r\n",
        "  if t % 100 ==99:\r\n",
        "    print(t ,loss)\r\n",
        "\r\n",
        "  # autograd를 사용하여 역전파 단계를 계산합니다.\r\n",
        "  loss.backward()\r\n",
        "\r\n",
        "  # 경사 하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\r\n",
        "  with torch.no_grad():\r\n",
        "    w1 -= learning_rate * w1.grad\r\n",
        "    w2 -= learning_rate * w2.grad\r\n",
        "\r\n",
        "    # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\r\n",
        "    w1.grad.zero_()\r\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}