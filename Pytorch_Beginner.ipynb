{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Propagation.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOION0pYQEGL"
      },
      "source": [
        "## 예제로 배우는 파이토치(Pytorch)\r\n",
        "\r\n",
        "### 특징\r\n",
        "\r\n",
        "- Numpy와 유사하지만 GPU 상에서 실행 가능한 N차원 Tensor\r\n",
        "- 신경망을 구성하고 학습하는 과정에서의 자동 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjigLVCWM_Qz"
      },
      "source": [
        "## Data Matrix\r\n",
        "\r\n",
        "<center>\r\n",
        "\r\n",
        "$ X = \\begin{bmatrix} x_{11} &...& x_{1p} \\\\ & \\vdots & \\\\ x\r\n",
        "_{n1}&  ... & x_{np}  \\end{bmatrix}  $ : $n \\times p$\r\n",
        "\r\n",
        "<br>\r\n",
        "$ Y = \\begin{bmatrix} y_{11} &...& y_{1k} \\\\ & \\vdots & \\\\ y\r\n",
        "_{n1}&  ... & y_{nk}  \\end{bmatrix}  $ : $n \\times k$\r\n",
        "\r\n",
        "where  $y_{ij} \\in \\{ 0,1 \\} $ and $\\sum_{j=1}^k y_{ij} = 1$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$ W_1 = \\begin{bmatrix} w_{111} & w_{112} &...& w_{11h} \\\\ & & \\vdots & \\\\ w\r\n",
        "_{1p1}& w_{1p2} & ... & w_{1ph}  \\end{bmatrix}  $ : $p \\times h$\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$ W_2 = \\begin{bmatrix} w_{211} & w_{212} &...& w_{21k} \\\\& & \\vdots & \\\\ w_{2h1}& w_{2h2} & ... & w_{2hk}  \\end{bmatrix}  $ : $h \\times k$\r\n",
        "\r\n",
        "</center>\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "## Feed Forward\r\n",
        "\r\n",
        "<center>\r\n",
        "<br>\r\n",
        "\r\n",
        "$ H = X \\cdot W_1$ : $n \\times h$\r\n",
        "\r\n",
        "$ H_{relu} = ReLU(H) $ : $n \\times h$\r\n",
        "\r\n",
        "$ Y_{pred} = H_{relu} \\cdot W_2 $ : $n \\times k$\r\n",
        "\r\n",
        "</center>\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "## Back Propagation\r\n",
        "\r\n",
        "<center>\r\n",
        "\r\n",
        "$ l(W_1, W_2) =  Tr[(Y_{pred} - Y )^T (Y_{pred} - Y)] = \\sum_{i=1}^{n} \\sum_{j=1}^{k} (y_{ij}^{pred} -y_{ij} )^2   $\r\n",
        "\r\n",
        "</center>\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "<center>\r\n",
        "\r\n",
        "$W_{2}^{(t)} = W_{2}^{(t-1)} + \\eta {\\partial l(W_1, W_2) \\over \\partial W_{2} }$\r\n",
        "\r\n",
        "${\\partial l(W_1, W_2) \\over \\partial W_{2} } = [ {\\partial Y_{pred} \\over \\partial W_{2}} ]^T \\cdot {\\partial l(W_1, W_2) \\over \\partial Y_{pred}}  = H_{relu}^T \\cdot 2(Y_{pred} - Y) $ : grad_w2\r\n",
        "\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$W_{1}^{(t)} = W_{1}^{(t-1)} + \\eta {\\partial l(W_1, W_2) \\over \\partial W_{1} }$\r\n",
        "\r\n",
        "${\\partial l(W_1, W_2) \\over \\partial W_1} =  [{\\partial H \\over \\partial W_1}]^T \\cdot  { H_{relu} \\over \\partial H} ( {\\partial l(W_1, W_2) \\over \\partial Y_{pred}} \\cdot [{\\partial Y_{pred}  \\over \\partial H_{relu}}]^T ) = [{\\partial H \\over \\partial W_1}]^T \\cdot  { H_{relu} \\over \\partial H}  $ (grad_h_relu)\r\n",
        "\r\n",
        "= $[{\\partial H \\over \\partial W_1}]^T \\cdot $ grad_h\r\n",
        "\r\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1v0UxWM-sH"
      },
      "source": [
        "# Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDS1x59kQPv3",
        "outputId": "8d509837-58e2-4311-ca93-c7306e2f5300"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다.\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "\r\n",
        "\r\n",
        "N, D_in, H, D_out = 64,1000,100,10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다\r\n",
        "\r\n",
        "x = np.random.randn(N, D_in)\r\n",
        "y = np.random.randn(N, D_out)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "\r\n",
        "w1 = np.random.randn(D_in , H)\r\n",
        "w2 = np.random.randn(H, D_out)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "\r\n",
        "for t in range(500):\r\n",
        "  # 순전파 단계 : 예측값 y를 계산합니다.\r\n",
        "  h = x.dot(w1) # N * H\r\n",
        "  h_relu = np.maximum(h,0) # N*H\r\n",
        "  y_pred = h_relu.dot(w2) # N*D_out\r\n",
        "\r\n",
        "  # 손실(loss)을 계산하고 출력합니다.\r\n",
        "  loss = np.square(y_pred - y).sum() # N * D_out\r\n",
        "  if t % 100 == 99:\r\n",
        "    print(t, loss)\r\n",
        "\r\n",
        "  # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "  grad_y_pred = 2.0 * (y_pred - y)  # N * D_out\r\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred) # H * D_out\r\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T) # N * H\r\n",
        "  grad_h = grad_h_relu.copy() # N * H\r\n",
        "  grad_h[h < 0] = 0\r\n",
        "  grad_w1 = x.T.dot(grad_h) # D_in * H\r\n",
        "\r\n",
        "  # 가중치를 갱신합니다.\r\n",
        "  w1 -= learning_rate * grad_w1\r\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 465.5093770879764\n",
            "199 1.6511792363910374\n",
            "299 0.00876622884089416\n",
            "399 5.146973608674881e-05\n",
            "499 3.1382471053474595e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGtBTMiSRR96"
      },
      "source": [
        "# PyTorch: Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBnJwjWw9y2",
        "outputId": "ce12fb86-2774-42e6-f087-dff0b4183477"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\r\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\r\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\r\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "for t in range(500):\r\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\r\n",
        "    h = x.mm(w1)\r\n",
        "    h_relu = h.clamp(min=0)\r\n",
        "    y_pred = h_relu.mm(w2)\r\n",
        "\r\n",
        "    # 손실(loss)을 계산하고 출력합니다.\r\n",
        "    loss = (y_pred - y).pow(2).sum().item()\r\n",
        "    if t % 100 == 99:\r\n",
        "        print(t, loss)\r\n",
        "\r\n",
        "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\r\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\r\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\r\n",
        "    grad_h = grad_h_relu.clone()\r\n",
        "    grad_h[h < 0] = 0\r\n",
        "    grad_w1 = x.t().mm(grad_h)\r\n",
        "\r\n",
        "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\r\n",
        "    w1 -= learning_rate * grad_w1\r\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 318.3465576171875\n",
            "199 0.8928102850914001\n",
            "299 0.004582075402140617\n",
            "399 0.0001473629818065092\n",
            "499 3.298965748399496e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp-JxXVWvpv9"
      },
      "source": [
        "# tensorflow : ?????"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL5Z09jGxX--",
        "outputId": "d4c1ecff-c087-4126-f560-373188a40cd2"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\r\n",
        "x = tf.random.normal([N, D_in], dtype=tf.float32)\r\n",
        "y = tf.random.normal([N, D_out], dtype=tf.float32)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "w1 = tf.Variable(tf.random.normal([D_in, H]), dtype=tf.float32)\r\n",
        "w2 = tf.Variable(tf.random.normal([H, D_out]), dtype=tf.float32)\r\n",
        "\r\n",
        "learning_rate = 1e-2\r\n",
        "\r\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\r\n",
        "optimizer = tf.keras.optimizers.SGD(lr = learning_rate)\r\n",
        "\r\n",
        "train_loss = tf.keras.metrics.Mean()\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\r\n",
        "    h = tf.matmul(x,w1)\r\n",
        "    h_relu = tf.maximum(h,0)\r\n",
        "    y_pred = tf.matmul(h_relu,w2)\r\n",
        "    # loss function\r\n",
        "    loss = loss_object(y_pred, y)  \r\n",
        "gradient = tape.gradient(loss, [w1,w2])\r\n",
        "optimizer.apply_gradients(zip(gradient, [w1,w2]))\r\n",
        "  \r\n",
        "\r\n",
        "# 반복 훈련\r\n",
        "for i in range(500):\r\n",
        "  if i % 10 == 1:\r\n",
        "    print(\"스텝 {:02d}에서 손실: {:.3f}\".format(i, loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "스텝 01에서 손실: 52388.297\n",
            "스텝 11에서 손실: 52388.297\n",
            "스텝 21에서 손실: 52388.297\n",
            "스텝 31에서 손실: 52388.297\n",
            "스텝 41에서 손실: 52388.297\n",
            "스텝 51에서 손실: 52388.297\n",
            "스텝 61에서 손실: 52388.297\n",
            "스텝 71에서 손실: 52388.297\n",
            "스텝 81에서 손실: 52388.297\n",
            "스텝 91에서 손실: 52388.297\n",
            "스텝 101에서 손실: 52388.297\n",
            "스텝 111에서 손실: 52388.297\n",
            "스텝 121에서 손실: 52388.297\n",
            "스텝 131에서 손실: 52388.297\n",
            "스텝 141에서 손실: 52388.297\n",
            "스텝 151에서 손실: 52388.297\n",
            "스텝 161에서 손실: 52388.297\n",
            "스텝 171에서 손실: 52388.297\n",
            "스텝 181에서 손실: 52388.297\n",
            "스텝 191에서 손실: 52388.297\n",
            "스텝 201에서 손실: 52388.297\n",
            "스텝 211에서 손실: 52388.297\n",
            "스텝 221에서 손실: 52388.297\n",
            "스텝 231에서 손실: 52388.297\n",
            "스텝 241에서 손실: 52388.297\n",
            "스텝 251에서 손실: 52388.297\n",
            "스텝 261에서 손실: 52388.297\n",
            "스텝 271에서 손실: 52388.297\n",
            "스텝 281에서 손실: 52388.297\n",
            "스텝 291에서 손실: 52388.297\n",
            "스텝 301에서 손실: 52388.297\n",
            "스텝 311에서 손실: 52388.297\n",
            "스텝 321에서 손실: 52388.297\n",
            "스텝 331에서 손실: 52388.297\n",
            "스텝 341에서 손실: 52388.297\n",
            "스텝 351에서 손실: 52388.297\n",
            "스텝 361에서 손실: 52388.297\n",
            "스텝 371에서 손실: 52388.297\n",
            "스텝 381에서 손실: 52388.297\n",
            "스텝 391에서 손실: 52388.297\n",
            "스텝 401에서 손실: 52388.297\n",
            "스텝 411에서 손실: 52388.297\n",
            "스텝 421에서 손실: 52388.297\n",
            "스텝 431에서 손실: 52388.297\n",
            "스텝 441에서 손실: 52388.297\n",
            "스텝 451에서 손실: 52388.297\n",
            "스텝 461에서 손실: 52388.297\n",
            "스텝 471에서 손실: 52388.297\n",
            "스텝 481에서 손실: 52388.297\n",
            "스텝 491에서 손실: 52388.297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fzCd4me6qYX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}