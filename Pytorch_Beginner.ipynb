{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Propagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimJH6647/Pytorch_Tutorial/blob/main/Pytorch_Beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOION0pYQEGL"
      },
      "source": [
        "## 예제로 배우는 파이토치(Pytorch)\r\n",
        "\r\n",
        "### 특징\r\n",
        "\r\n",
        "- Numpy와 유사하지만 GPU 상에서 실행 가능한 N차원 Tensor\r\n",
        "- 신경망을 구성하고 학습하는 과정에서의 자동 미분"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjigLVCWM_Qz"
      },
      "source": [
        "$$ W_1 = \\begin{bmatrix} w_{111} & w_{112} &...& w_{1,1,100} \\\\ & & \\vdots & \\\\ w_{1,1000, 1}& w_{1,1000, 2} & ... & w_{1,1000, 100}  \\end{bmatrix}  $$\r\n",
        "\r\n",
        "$$ W_2 = \\begin{bmatrix} w_{211} & w_{212} &...& w_{2,1,10} \\\\& & \\vdots & \\\\ w_{2,100, 1}& w_{2, 100, 2} & ... & w_{2, 100, 10}  \\end{bmatrix}  $$\r\n",
        "\r\n",
        "\r\n",
        "$$ l(w_1, w_2) = \\sum_{i=1}^{n} \\sum_{j=1}^{10} (y_{ij} - y_{ij}^{pred})^2   $$\r\n",
        "\r\n",
        "$$ h = X \\cdot w_1$$\r\n",
        "\r\n",
        "$$ h^{relu} = ReLU(h) $$\r\n",
        "\r\n",
        "$$ y^{pred} = h^{relu} \\cdot w_2$$\r\n",
        "\r\n",
        "$${\\partial l(w_1, w_2) \\over \\partial w_2} = {\\partial l(w_1, w_2) \\over \\partial y^{pred}} {\\partial y^{pred} \\over \\partial w_2}  $$\r\n",
        "\r\n",
        "\r\n",
        "$${\\partial l(w_1, w_2) \\over \\partial w_1} = {\\partial l(w_1, w_2) \\over \\partial y^{pred}} {\\partial y^{pred} \\over \\partial h^{relu}} { h^{relu} \\over \\partial h} {\\partial h \\over \\partial w_1}  $$\r\n",
        "\r\n",
        "??????????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1v0UxWM-sH"
      },
      "source": [
        "# Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDS1x59kQPv3",
        "outputId": "8d509837-58e2-4311-ca93-c7306e2f5300"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다.\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "\r\n",
        "N, D_in, H, D_out = 64,1000,100,10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다\r\n",
        "\r\n",
        "x = np.random.randn(N, D_in)\r\n",
        "y = np.random.randn(N, D_out)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "\r\n",
        "w1 = np.random.randn(D_in , H)\r\n",
        "w2 = np.random.randn(H, D_out)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "\r\n",
        "for t in range(500):\r\n",
        "  # 순전파 단계 : 예측값 y를 계산합니다.\r\n",
        "  h = x.dot(w1) # N * H\r\n",
        "  h_relu = np.maximum(h,0) # N*H\r\n",
        "  y_pred = h_relu.dot(w2) # N*D_out\r\n",
        "\r\n",
        "  # 손실(loss)을 계산하고 출력합니다.\r\n",
        "  loss = np.square(y_pred - y).sum() # N * D_out\r\n",
        "  if t % 100 == 99:\r\n",
        "    print(t, loss)\r\n",
        "\r\n",
        "  # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "  grad_y_pred = 2.0 * (y_pred - y)  # N * D_out\r\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred) # H * D_out\r\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T) # N * H\r\n",
        "  grad_h = grad_h_relu.copy() # N * H\r\n",
        "  grad_h[h < 0] = 0\r\n",
        "  grad_w1 = x.T.dot(grad_h) # D_in * H\r\n",
        "\r\n",
        "  # 가중치를 갱신합니다.\r\n",
        "  w1 -= learning_rate * grad_w1\r\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 465.5093770879764\n",
            "199 1.6511792363910374\n",
            "299 0.00876622884089416\n",
            "399 5.146973608674881e-05\n",
            "499 3.1382471053474595e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGtBTMiSRR96"
      },
      "source": [
        "# PyTorch: Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgBnJwjWw9y2",
        "outputId": "ce12fb86-2774-42e6-f087-dff0b4183477"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "dtype = torch.float\r\n",
        "device = torch.device(\"cpu\")\r\n",
        "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\r\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\r\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\r\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\r\n",
        "\r\n",
        "learning_rate = 1e-6\r\n",
        "for t in range(500):\r\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\r\n",
        "    h = x.mm(w1)\r\n",
        "    h_relu = h.clamp(min=0)\r\n",
        "    y_pred = h_relu.mm(w2)\r\n",
        "\r\n",
        "    # 손실(loss)을 계산하고 출력합니다.\r\n",
        "    loss = (y_pred - y).pow(2).sum().item()\r\n",
        "    if t % 100 == 99:\r\n",
        "        print(t, loss)\r\n",
        "\r\n",
        "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\r\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\r\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\r\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\r\n",
        "    grad_h = grad_h_relu.clone()\r\n",
        "    grad_h[h < 0] = 0\r\n",
        "    grad_w1 = x.t().mm(grad_h)\r\n",
        "\r\n",
        "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\r\n",
        "    w1 -= learning_rate * grad_w1\r\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 318.3465576171875\n",
            "199 0.8928102850914001\n",
            "299 0.004582075402140617\n",
            "399 0.0001473629818065092\n",
            "499 3.298965748399496e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp-JxXVWvpv9"
      },
      "source": [
        "# tensorflow : ?????"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sL5Z09jGxX--",
        "outputId": "d4c1ecff-c087-4126-f560-373188a40cd2"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\r\n",
        "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\r\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\r\n",
        "\r\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\r\n",
        "x = tf.random.normal([N, D_in], dtype=tf.float32)\r\n",
        "y = tf.random.normal([N, D_out], dtype=tf.float32)\r\n",
        "\r\n",
        "# 무작위로 가중치를 초기화합니다.\r\n",
        "w1 = tf.Variable(tf.random.normal([D_in, H]), dtype=tf.float32)\r\n",
        "w2 = tf.Variable(tf.random.normal([H, D_out]), dtype=tf.float32)\r\n",
        "\r\n",
        "learning_rate = 1e-2\r\n",
        "\r\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\r\n",
        "optimizer = tf.keras.optimizers.SGD(lr = learning_rate)\r\n",
        "\r\n",
        "train_loss = tf.keras.metrics.Mean()\r\n",
        "\r\n",
        "with tf.GradientTape() as tape:\r\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\r\n",
        "    h = tf.matmul(x,w1)\r\n",
        "    h_relu = tf.maximum(h,0)\r\n",
        "    y_pred = tf.matmul(h_relu,w2)\r\n",
        "    # loss function\r\n",
        "    loss = loss_object(y_pred, y)  \r\n",
        "gradient = tape.gradient(loss, [w1,w2])\r\n",
        "optimizer.apply_gradients(zip(gradient, [w1,w2]))\r\n",
        "  \r\n",
        "\r\n",
        "# 반복 훈련\r\n",
        "for i in range(500):\r\n",
        "  if i % 10 == 1:\r\n",
        "    print(\"스텝 {:02d}에서 손실: {:.3f}\".format(i, loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "스텝 01에서 손실: 52388.297\n",
            "스텝 11에서 손실: 52388.297\n",
            "스텝 21에서 손실: 52388.297\n",
            "스텝 31에서 손실: 52388.297\n",
            "스텝 41에서 손실: 52388.297\n",
            "스텝 51에서 손실: 52388.297\n",
            "스텝 61에서 손실: 52388.297\n",
            "스텝 71에서 손실: 52388.297\n",
            "스텝 81에서 손실: 52388.297\n",
            "스텝 91에서 손실: 52388.297\n",
            "스텝 101에서 손실: 52388.297\n",
            "스텝 111에서 손실: 52388.297\n",
            "스텝 121에서 손실: 52388.297\n",
            "스텝 131에서 손실: 52388.297\n",
            "스텝 141에서 손실: 52388.297\n",
            "스텝 151에서 손실: 52388.297\n",
            "스텝 161에서 손실: 52388.297\n",
            "스텝 171에서 손실: 52388.297\n",
            "스텝 181에서 손실: 52388.297\n",
            "스텝 191에서 손실: 52388.297\n",
            "스텝 201에서 손실: 52388.297\n",
            "스텝 211에서 손실: 52388.297\n",
            "스텝 221에서 손실: 52388.297\n",
            "스텝 231에서 손실: 52388.297\n",
            "스텝 241에서 손실: 52388.297\n",
            "스텝 251에서 손실: 52388.297\n",
            "스텝 261에서 손실: 52388.297\n",
            "스텝 271에서 손실: 52388.297\n",
            "스텝 281에서 손실: 52388.297\n",
            "스텝 291에서 손실: 52388.297\n",
            "스텝 301에서 손실: 52388.297\n",
            "스텝 311에서 손실: 52388.297\n",
            "스텝 321에서 손실: 52388.297\n",
            "스텝 331에서 손실: 52388.297\n",
            "스텝 341에서 손실: 52388.297\n",
            "스텝 351에서 손실: 52388.297\n",
            "스텝 361에서 손실: 52388.297\n",
            "스텝 371에서 손실: 52388.297\n",
            "스텝 381에서 손실: 52388.297\n",
            "스텝 391에서 손실: 52388.297\n",
            "스텝 401에서 손실: 52388.297\n",
            "스텝 411에서 손실: 52388.297\n",
            "스텝 421에서 손실: 52388.297\n",
            "스텝 431에서 손실: 52388.297\n",
            "스텝 441에서 손실: 52388.297\n",
            "스텝 451에서 손실: 52388.297\n",
            "스텝 461에서 손실: 52388.297\n",
            "스텝 471에서 손실: 52388.297\n",
            "스텝 481에서 손실: 52388.297\n",
            "스텝 491에서 손실: 52388.297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fzCd4me6qYX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}